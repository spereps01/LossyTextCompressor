{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. IMPORTAZIONE LIBRERIE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vec2text\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Sopprimiamo i warning\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. FUNZIONI PER OTTENERE EMBEDDING E QUANTIZZAZIONI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per ottenere embedding utilizzando un modello GTR (Generalized T5 Retrieval)\n",
    "def get_gtr_embeddings(text_list, encoder, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text_list,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = encoder(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        hidden_state = model_output.last_hidden_state\n",
    "        embeddings = vec2text.models.model_utils.mean_pool(hidden_state, inputs['attention_mask'])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Funzione per quantizzare gli embedding in torch.uint8\n",
    "def quantize_embeddings(embeddings, bits=8):\n",
    "    scale = 2 ** bits - 1\n",
    "    embeddings_min = embeddings.min(dim=-1, keepdim=True).values\n",
    "    embeddings_max = embeddings.max(dim=-1, keepdim=True).values\n",
    "    quantized = ((embeddings - embeddings_min) / (embeddings_max - embeddings_min)) * scale\n",
    "    quantized = quantized.round().to(torch.uint8)  # Convertiamo a uint8\n",
    "    return quantized, embeddings_min, embeddings_max\n",
    "\n",
    "# Funzione per de-quantizzare gli embeddings\n",
    "def dequantize_embeddings(quantized, embeddings_min, embeddings_max, bits=8):\n",
    "    scale = 2 ** bits - 1\n",
    "    quantized = quantized.to(torch.float32)  # Convertiamo a float32 per operazioni\n",
    "    dequantized = (quantized / scale) * (embeddings_max - embeddings_min) + embeddings_min\n",
    "    return dequantized\n",
    "\n",
    "# Funzione per calcolare la percentuale di compressione\n",
    "def calculate_compression_percentage(original_embeddings, quantized_embeddings):\n",
    "    original_size = original_embeddings.numel() * original_embeddings.element_size()\n",
    "    quantized_size = quantized_embeddings.numel() * quantized_embeddings.element_size()\n",
    "    compression_ratio = (original_size - quantized_size) / original_size * 100\n",
    "    return compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CARICAMENTO MODELLI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at sentence-transformers/gtr-t5-base and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce2fd2383dd4ee198a8e7d241a9265d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f011ec776541b898b8ae54faf0f1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carica il modello di encoder GTR e il tokenizer\n",
    "encoder = AutoModel.from_pretrained(\"sentence-transformers/gtr-t5-base\").encoder.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/gtr-t5-base\")\n",
    "\n",
    "# Carica il modello di corrector preaddestrato\n",
    "corrector = vec2text.load_pretrained_corrector(\"gtr-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. TESTI ED EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In a quiet corner of the city, hidden among the busy streets and modern skyscrapers, lies a small park that offers an oasis of peace to residents', ' This park, once part of a large private estate, was redeveloped and opened to the public in the 1980s', ' Today, its green spaces, tree-lined paths, and serene ponds attract families, joggers, and nature enthusiasts', ' Visitors can stroll along trails bordered by colorful flowers, relax on benches situated beneath ancient trees, or simply enjoy the tranquility of the landscape', ' Each year, the park hosts community events and local festivals that celebrate the culture and traditions of the neighborhood', ' Despite the increasing urbanization around it, this slice of nature remains a cherished refuge for those seeking a moment of relaxation and connection with the natural world']\n"
     ]
    }
   ],
   "source": [
    "# Lista di frasi da convertire in embedding e poi invertire\n",
    "text = \"In a quiet corner of the city, hidden among the busy streets and modern skyscrapers, lies a small park that offers an oasis of peace to residents. This park, once part of a large private estate, was redeveloped and opened to the public in the 1980s. Today, its green spaces, tree-lined paths, and serene ponds attract families, joggers, and nature enthusiasts. Visitors can stroll along trails bordered by colorful flowers, relax on benches situated beneath ancient trees, or simply enjoy the tranquility of the landscape. Each year, the park hosts community events and local festivals that celebrate the culture and traditions of the neighborhood. Despite the increasing urbanization around it, this slice of nature remains a cherished refuge for those seeking a moment of relaxation and connection with the natural world.\"\n",
    "# Ottieni gli embedding dalle frasi\n",
    "text_list = text.split('.')[:-1]\n",
    "print (text_list)\n",
    "embeddings = get_gtr_embeddings(text_list, encoder, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. ESECUZIONE OPERAZIONI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicare quantizzazione agli embedding\n",
    "quantized_embeddings, embeddings_min, embeddings_max = quantize_embeddings(embeddings, bits=8)\n",
    "\n",
    "# Dequantizzare gli embeddings per ricostruire il testo\n",
    "dequantized_embeddings = dequantize_embeddings(quantized_embeddings, embeddings_min, embeddings_max, bits=8)\n",
    "\n",
    "# Inversione degli embedding quantizzati per ricostruire il testo\n",
    "inverted_texts = vec2text.invert_embeddings(\n",
    "    embeddings=dequantized_embeddings.cuda(),\n",
    "    corrector=corrector,\n",
    "    num_steps=20,\n",
    "    sequence_beam_width=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. STAMPA RISULTATI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: In a quiet corner of the city, hidden among the busy streets and modern skyscrapers, lies a small park that offers an oasis of peace to residents.  This park, once part of a large private estate, was redeveloped and opened to the public in the 1980s.  Today, its green spaces, tree-lined paths, and serene ponds attract families, joggers, and nature enthusiasts.  Visitors can stroll along trails bordered by colorful flowers, relax on benches situated beneath ancient trees, or simply enjoy the tranquility of the landscape.  Each year, the park hosts community events and local festivals that celebrate the culture and traditions of the neighborhood.  Despite the increasing urbanization around it, this slice of nature remains a cherished refuge for those seeking a moment of relaxation and connection with the natural world. \n",
      "Quantized: In a quiet corner of the city, hidden among the busy streets and modern skyscrapers, lies a small park that offers peace to residents. This park, once part of a large private estate,   was redeveloped and opened to the public in the 1980s. Today, its green spaces, tree-lined paths, and serene ponds attract families, joggers, and nature enthusiasts . Visitors can stroll along trails bordered by colorful flowers, relax on benches situated beneath ancient trees, or simply enjoy the tranquility of the landscape . Each year, the park hosts community events and local festivals that celebrate the culture      and traditions of the neighborhood . Despite the increasing urbanization around it, this slice of nature remains a cherished refuge for those seeking a connection and relaxation with the natural world. \n",
      "Compression Percentage: 75.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stampa i risultati dell'inversione e la percentuale di compressione\n",
    "or_seq =''\n",
    "inv_seq =''\n",
    "\n",
    "for original, inverted, orig_emb, quant_emb in zip(text_list, inverted_texts, embeddings, quantized_embeddings):\n",
    "    compression_percentage = calculate_compression_percentage(orig_emb, quant_emb)\n",
    "    or_seq += original +'. '\n",
    "    inv_seq +=inverted + '. '\n",
    "print(f\"Original: {or_seq}\")\n",
    "print(f\"Quantized: {inv_seq}\")\n",
    "print(f\"Compression Percentage: {compression_percentage:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. VALUTAZIONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.9972832202911377\n",
      "Average BLEU Score: 0.9310088782608926\n",
      "Average ROUGE Scores: {'rouge-1': 0.9892075188995784, 'rouge-2': 0.9496598589500352, 'rouge-l': 0.9750231217364577}\n"
     ]
    }
   ],
   "source": [
    "# Valutazione della qualità della ricostruzione\n",
    "def calculate_cosine_similarity(original_texts, inverted_texts):\n",
    "    original_embeddings = get_gtr_embeddings(original_texts, encoder, tokenizer)\n",
    "    inverted_embeddings = get_gtr_embeddings(inverted_texts, encoder, tokenizer)\n",
    "    \n",
    "    # Calcola la similarità del coseno per ogni coppia di embedding\n",
    "    cosine_similarities = []\n",
    "    for orig_emb, inv_emb in zip(original_embeddings, inverted_embeddings):\n",
    "        similarity = cosine_similarity(orig_emb.cpu().numpy().reshape(1, -1), inv_emb.cpu().numpy().reshape(1, -1))\n",
    "        cosine_similarities.append(similarity[0][0])\n",
    "    \n",
    "    return np.mean(cosine_similarities)\n",
    "\n",
    "def calculate_bleu_score(original_texts, inverted_texts):\n",
    "    bleu_scores = []\n",
    "    for orig, inv in zip(original_texts, inverted_texts):\n",
    "        reference = orig.split()\n",
    "        candidate = inv.split()\n",
    "        bleu_score = sentence_bleu([reference], candidate)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    \n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "def calculate_rouge_score(original_texts, inverted_texts):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = []\n",
    "    for orig, inv in zip(original_texts, inverted_texts):\n",
    "        scores = rouge.get_scores(inv, orig, avg=True)\n",
    "        rouge_scores.append(scores)\n",
    "\n",
    "    avg_rouge_scores = {\n",
    "        'rouge-1': np.mean([score['rouge-1']['f'] for score in rouge_scores]),\n",
    "        'rouge-2': np.mean([score['rouge-2']['f'] for score in rouge_scores]),\n",
    "        'rouge-l': np.mean([score['rouge-l']['f'] for score in rouge_scores])\n",
    "    }\n",
    "    \n",
    "    return avg_rouge_scores\n",
    "\n",
    "# Calcola le metriche di valutazione\n",
    "cosine_similarity_score = calculate_cosine_similarity(text_list, inverted_texts)\n",
    "bleu_score = calculate_bleu_score(text_list, inverted_texts)\n",
    "\n",
    "rouge_scores = calculate_rouge_score(text_list, inverted_texts)\n",
    "\n",
    "print(f\"Average Cosine Similarity: {cosine_similarity_score}\")\n",
    "print(f\"Average BLEU Score: {bleu_score}\")\n",
    "print(f\"Average ROUGE Scores: {rouge_scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERIFICHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between original and quantized embeddings: 0.864703\n"
     ]
    }
   ],
   "source": [
    "# Verifica della differenza tra embeddings originali e quantizzati\n",
    "def check_embedding_difference(original_embeddings, quantized_embeddings):\n",
    "    difference = torch.abs(original_embeddings - quantized_embeddings).sum().item()\n",
    "    return difference\n",
    "\n",
    "difference = check_embedding_difference(embeddings, dequantized_embeddings)\n",
    "print(f\"Difference between original and quantized embeddings: {difference:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
