{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. IMPORTAZIONE LIBRERIE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vec2text\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Sopprimiamo i warning\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. FUNZIONI PER OTTENERE EMBEDDING E QUANTIZZAZIONI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per ottenere embedding utilizzando un modello GTR (Generalized T5 Retrieval)\n",
    "def get_gtr_embeddings(text_list, encoder, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        text_list,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_output = encoder(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "        hidden_state = model_output.last_hidden_state\n",
    "        embeddings = vec2text.models.model_utils.mean_pool(hidden_state, inputs['attention_mask'])\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Funzione per quantizzare gli embedding in torch.uint8\n",
    "def quantize_embeddings(embeddings, bits=8):\n",
    "    scale = 2 ** bits - 1\n",
    "    embeddings_min = embeddings.min(dim=-1, keepdim=True).values\n",
    "    embeddings_max = embeddings.max(dim=-1, keepdim=True).values\n",
    "    quantized = ((embeddings - embeddings_min) / (embeddings_max - embeddings_min)) * scale\n",
    "    quantized = quantized.round().to(torch.uint8)  # Convertiamo a uint8\n",
    "    return quantized, embeddings_min, embeddings_max\n",
    "\n",
    "# Funzione per de-quantizzare gli embeddings\n",
    "def dequantize_embeddings(quantized, embeddings_min, embeddings_max, bits=8):\n",
    "    scale = 2 ** bits - 1\n",
    "    quantized = quantized.to(torch.float32)  # Convertiamo a float32 per operazioni\n",
    "    dequantized = (quantized / scale) * (embeddings_max - embeddings_min) + embeddings_min\n",
    "    return dequantized\n",
    "\n",
    "# Funzione per calcolare la percentuale di compressione\n",
    "def calculate_compression_percentage(original_embeddings, quantized_embeddings):\n",
    "    original_size = original_embeddings.numel() * original_embeddings.element_size()\n",
    "    quantized_size = quantized_embeddings.numel() * quantized_embeddings.element_size()\n",
    "    compression_ratio = (original_size - quantized_size) / original_size * 100\n",
    "    return compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CARICAMENTO MODELLI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at sentence-transformers/gtr-t5-base and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.final_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65564064474b43c0ac63a66219514e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f119706257c436795e187cfe7516d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carica il modello di encoder GTR e il tokenizer\n",
    "encoder = AutoModel.from_pretrained(\"sentence-transformers/gtr-t5-base\").encoder.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/gtr-t5-base\")\n",
    "\n",
    "# Carica il modello di corrector preaddestrato\n",
    "corrector = vec2text.load_pretrained_corrector(\"gtr-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. TESTI ED EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista di frasi da convertire in embedding e poi invertire\n",
    "text_list = [\n",
    "    \"My name is John Smith, I am 19 and a student in the college of New York. My favorite courses are Geometry and French.\",\n",
    "    \"The conference will be held on September 15th, 2024, at the Hilton Hotel, with over 300 attendees expected.\",\n",
    "\"The company reported a 12% increase in revenue for the second quarter, surpassing analysts' expectations.\",\n",
    "\"The rocket launch is scheduled for 10:30 AM, with a payload consisting of scientific instruments and satellites.\"\n",
    "]\n",
    "\n",
    "# Ottieni gli embedding dalle frasi\n",
    "embeddings = get_gtr_embeddings(text_list, encoder, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. ESECUZIONE OPERAZIONI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicare quantizzazione agli embedding\n",
    "quantized_embeddings, embeddings_min, embeddings_max = quantize_embeddings(embeddings, bits=8)\n",
    "\n",
    "# Dequantizzare gli embeddings per ricostruire il testo\n",
    "dequantized_embeddings = dequantize_embeddings(quantized_embeddings, embeddings_min, embeddings_max, bits=8)\n",
    "\n",
    "# Inversione degli embedding quantizzati per ricostruire il testo\n",
    "inverted_texts = vec2text.invert_embeddings(\n",
    "    embeddings=dequantized_embeddings.cuda(),\n",
    "    corrector=corrector,\n",
    "    num_steps=20,\n",
    "    sequence_beam_width=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. STAMPA RISULTATI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: My name is John Smith, I am 19 and a student in the college of New York. My favorite courses are Geometry and French.\n",
      "Inverted (Quantized): My name is John Smith, I am 19 and a student in the college of New York. My favorite courses are Geometry and French. \n",
      "Compression Percentage: 75.00%\n",
      "\n",
      "Original: The conference will be held on September 15th, 2024, at the Hilton Hotel, with over 300 attendees expected.\n",
      "Inverted (Quantized): The conference will be held on September 15th, 2024, at the Hilton Hotel,    with over 300 attendees expected. \n",
      "Compression Percentage: 75.00%\n",
      "\n",
      "Original: The company reported a 12% increase in revenue for the second quarter, surpassing analysts' expectations.\n",
      "Inverted (Quantized): The company reported a     12% increase in revenue for the second quarter, surpassing analysts' expectations. \n",
      "Compression Percentage: 75.00%\n",
      "\n",
      "Original: The rocket launch is scheduled for 10:30 AM, with a payload consisting of scientific instruments and satellites.\n",
      "Inverted (Quantized): The rocket launch is scheduled for 10:30    AM, with a payload consisting of scientific instruments and satellites.\n",
      "Compression Percentage: 75.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stampa i risultati dell'inversione e la percentuale di compressione\n",
    "for original, inverted, orig_emb, quant_emb in zip(text_list, inverted_texts, embeddings, quantized_embeddings):\n",
    "    compression_percentage = calculate_compression_percentage(orig_emb, quant_emb)\n",
    "    print(f\"Original: {original}\")\n",
    "    print(f\"Inverted (Quantized): {inverted}\")\n",
    "    print(f\"Compression Percentage: {compression_percentage:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. VALUTAZIONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity: 0.9999998807907104\n",
      "Average BLEU Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Valutazione della qualità della ricostruzione\n",
    "def calculate_cosine_similarity(original_texts, inverted_texts):\n",
    "    original_embeddings = get_gtr_embeddings(original_texts, encoder, tokenizer)\n",
    "    inverted_embeddings = get_gtr_embeddings(inverted_texts, encoder, tokenizer)\n",
    "    \n",
    "    # Calcola la similarità del coseno per ogni coppia di embedding\n",
    "    cosine_similarities = []\n",
    "    for orig_emb, inv_emb in zip(original_embeddings, inverted_embeddings):\n",
    "        similarity = cosine_similarity(orig_emb.cpu().numpy().reshape(1, -1), inv_emb.cpu().numpy().reshape(1, -1))\n",
    "        cosine_similarities.append(similarity[0][0])\n",
    "    \n",
    "    return np.mean(cosine_similarities)\n",
    "\n",
    "def calculate_bleu_score(original_texts, inverted_texts):\n",
    "    bleu_scores = []\n",
    "    for orig, inv in zip(original_texts, inverted_texts):\n",
    "        reference = orig.split()\n",
    "        candidate = inv.split()\n",
    "        bleu_score = sentence_bleu([reference], candidate)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    \n",
    "    return np.mean(bleu_scores)\n",
    "\n",
    "# Calcola le metriche di valutazione\n",
    "cosine_similarity_score = calculate_cosine_similarity(text_list, inverted_texts)\n",
    "bleu_score = calculate_bleu_score(text_list, inverted_texts)\n",
    "\n",
    "print(f\"Average Cosine Similarity: {cosine_similarity_score}\")\n",
    "print(f\"Average BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERIFICHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between original and quantized embeddings: 0.864703\n"
     ]
    }
   ],
   "source": [
    "# Verifica della differenza tra embeddings originali e quantizzati\n",
    "def check_embedding_difference(original_embeddings, quantized_embeddings):\n",
    "    difference = torch.abs(original_embeddings - quantized_embeddings).sum().item()\n",
    "    return difference\n",
    "\n",
    "difference = check_embedding_difference(embeddings, dequantized_embeddings)\n",
    "print(f\"Difference between original and quantized embeddings: {difference:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
